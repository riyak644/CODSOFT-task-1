{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1tlSrsi__tW",
        "outputId": "cbfce1a0-7c7d-4239-84fe-cd7c96f1b3e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: geopy in /usr/local/lib/python3.11/dist-packages (2.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.11/dist-packages (from geopy) (2.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
            "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
            "  T = new_sum / new_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
            "  new_unnormalized_variance -= correction**2 / new_sample_count\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [1]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [1]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Logistic Regression - AUC: 0.4998\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    110715\n",
            "           1       0.00      0.00      0.00       429\n",
            "\n",
            "    accuracy                           1.00    111144\n",
            "   macro avg       0.50      0.50      0.50    111144\n",
            "weighted avg       0.99      1.00      0.99    111144\n",
            "\n",
            "[[110681     34]\n",
            " [   429      0]]\n",
            "\n",
            "Decision Tree - AUC: 0.7828\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    110715\n",
            "           1       0.73      0.57      0.64       429\n",
            "\n",
            "    accuracy                           1.00    111144\n",
            "   macro avg       0.86      0.78      0.82    111144\n",
            "weighted avg       1.00      1.00      1.00    111144\n",
            "\n",
            "[[110624     91]\n",
            " [   186    243]]\n",
            "\n",
            "Random Forest - AUC: 0.5000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00    110715\n",
            "           1       0.00      0.00      0.00       429\n",
            "\n",
            "    accuracy                           1.00    111144\n",
            "   macro avg       0.50      0.50      0.50    111144\n",
            "weighted avg       0.99      1.00      0.99    111144\n",
            "\n",
            "[[110715      0]\n",
            " [   429      0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "!pip install geopy scikit-learn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from geopy.distance import geodesic\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/fraudTest.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "cols_to_drop = ['Unnamed: 0', 'cc_num', 'first', 'last', 'street', 'city', 'state', 'zip', 'trans_num', 'dob', 'trans_date_trans_time']\n",
        "data.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "data['gender'] = label_encoder.fit_transform(data['gender'])\n",
        "data['category'] = label_encoder.fit_transform(data['category'])\n",
        "data['merchant'] = label_encoder.fit_transform(data['merchant'])\n",
        "\n",
        "# Create distance feature (distance between transaction and merchant)\n",
        "def calculate_distance(row):\n",
        "   if pd.isna(row['lat']) or pd.isna(row['long']) or pd.isna(row['merch_lat']) or pd.isna(row['merch_long']):\n",
        "        return 0\n",
        "        user_location = (row['lat'], row['long'])\n",
        "        merch_location = (row['merch_lat'], row['merch_long'])\n",
        "        return geodesic(user_location, merch_location).kilometers\n",
        "\n",
        "data['distance'] = data.apply(calculate_distance, axis=1)\n",
        "data.drop(columns=['lat', 'long', 'merch_lat', 'merch_long'], inplace=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop(columns=['is_fraud'])\n",
        "y = data['is_fraud']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Define categorical and numerical features\n",
        "categorical_features = ['gender', 'category', 'merchant']\n",
        "numerical_features = ['amt', 'distance']\n",
        "\n",
        "# Create preprocessing pipelines for categorical and numerical features\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore')), # Use OneHotEncoder to handle categorical features\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent'))\n",
        "])\n",
        "\n",
        "numerical_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Combine pipelines using ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Apply preprocessing to training and testing data\n",
        "X_train = preprocessor.fit_transform(X_train)\n",
        "X_test = preprocessor.transform(X_test)\n",
        "\n",
        "# Impute any remaining NaNs after preprocessing\n",
        "imputer = SimpleImputer(strategy='mean') # Use mean imputation for any remaining NaNs\n",
        "X_train = imputer.fit_transform(X_train)\n",
        "X_test = imputer.transform(X_test)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    auc = roc_auc_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    results[name] = {'AUC': auc, 'Report': report, 'Confusion Matrix': cm}\n",
        "    print(f\"\\n{name} - AUC: {auc:.4f}\")\n",
        "    print(report)\n",
        "    print(cm)\n"
      ]
    }
  ]
}